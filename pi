#!/usr/bin/env node
/**
 * Prime Intellect CLI - All-in-one pod management
 */

const fs = require('fs');
const { execSync, spawn } = require('child_process');
const path = require('path');
const os = require('os');

const CONFIG_FILE = path.join(os.homedir(), '.pi_config');
const SCRIPT_DIR = __dirname;

class PrimeIntellectCLI {
    constructor() {
        this.loadConfig();
    }

    loadConfig() {
        if (fs.existsSync(CONFIG_FILE)) {
            this.config = JSON.parse(fs.readFileSync(CONFIG_FILE, 'utf8'));
            // Migrate old single-pod config
            if (this.config.ssh && !this.config.pods) {
                this.config = {
                    pods: { 'default': { ssh: this.config.ssh } },
                    active: 'default'
                };
                this.saveConfig();
            }
        } else {
            this.config = { pods: {}, active: null };
        }
    }

    saveConfig() {
        fs.writeFileSync(CONFIG_FILE, JSON.stringify(this.config, null, 2));
    }

    getActivePod() {
        if (!this.config.active || !this.config.pods[this.config.active]) {
            return null;
        }
        return this.config.pods[this.config.active];
    }

    ssh(command, interactive = false, skipPirc = false) {
        const pod = this.getActivePod();
        if (!pod) {
            console.error('No active pod. Run: pi setup <pod-name> <ssh_command>');
            console.error('Example: pi setup prod "root@135.181.71.41 -p 22"');
            console.error('Or activate an existing pod: pi pod <pod-name>');
            process.exit(1);
        }

        // Wrap command to source .pirc first (if it exists), unless skipPirc is true
        const finalCommand = skipPirc ? command : `[ -f ~/.pirc ] && source ~/.pirc; ${command}`;

        if (interactive) {
            // For interactive commands, use spawn with shell
            const sshParts = pod.ssh.split(' ');
            const sshCmd = ['ssh', ...sshParts, finalCommand];
            const proc = spawn(sshCmd[0], sshCmd.slice(1), { stdio: 'inherit', shell: false });
            return new Promise((resolve) => {
                proc.on('close', resolve);
            });
        } else {
            const sshCmd = `ssh ${pod.ssh} ${JSON.stringify(finalCommand)}`;

            // For non-interactive, use execSync
            try {
                return execSync(sshCmd, { encoding: 'utf8' });
            } catch (e) {
                if (e.status !== 0) {
                    console.error('SSH command failed:', e.message);
                    process.exit(1);
                }
                throw e;
            }
        }
    }

    scp(localFile, remotePath = '~/') {
        const pod = this.getActivePod();
        if (!pod) {
            console.error('No active pod. Run: pi setup <pod-name> <ssh_command>');
            process.exit(1);
        }

        const [userHost, ...sshArgs] = pod.ssh.split(' ');
        let scpCmd = `scp`;

        // Add port if specified
        const portArg = sshArgs.find(arg => arg === '-p');
        if (portArg) {
            const portIndex = sshArgs.indexOf(portArg);
            const port = sshArgs[portIndex + 1];
            scpCmd += ` -P ${port}`;
        }

        scpCmd += ` ${localFile} ${userHost}:${remotePath}`;

        try {
            execSync(scpCmd, { stdio: 'inherit' });
        } catch (e) {
            console.error('SCP failed:', e.message);
            process.exit(1);
        }
    }

    async setup(podName, sshCommand) {
        if (!podName || !sshCommand) {
            console.error('Usage: pi setup <pod-name> <ssh_command>');
            console.error('Example: pi setup prod "root@135.181.71.41 -p 22"');
            process.exit(1);
        }

        // Remove "ssh " prefix if present
        if (sshCommand.toLowerCase().startsWith('ssh ')) {
            sshCommand = sshCommand.substring(4);
        }

        // Save pod config
        if (!this.config.pods) {
            this.config.pods = {};
        }
        this.config.pods[podName] = { ssh: sshCommand };
        this.config.active = podName;
        this.saveConfig();
        console.log(`Saved pod '${podName}' with SSH: ${sshCommand}`);

        // Test connection
        console.log('\nTesting SSH connection...');
        try {
            const hostname = this.ssh('hostname', false, true).trim();
            console.log(`✓ Connected to ${hostname}`);
        } catch (e) {
            console.error('✗ SSH connection failed');
            process.exit(1);
        }

        // Copy setup files
        console.log('\nCopying setup files...');
        this.scp(path.join(SCRIPT_DIR, 'pod_setup.sh'));
        this.scp(path.join(SCRIPT_DIR, 'vllm_manager.py'));

        // Run setup with HF_TOKEN
        console.log('\nRunning setup script...');
        const hfToken = process.env.HF_TOKEN;
        if (!hfToken) {
            console.error('\nERROR: HF_TOKEN environment variable not set');
            console.error('Please export HF_TOKEN before running setup');
            process.exit(1);
        }
        await this.ssh(`export HF_TOKEN="${hfToken}" && bash pod_setup.sh`, true, true);

        console.log('\n✓ Setup complete!');

        // Show usage help
        this.showHelp();
    }

    list() {
        const output = this.ssh('python3 vllm_manager.py list');
        console.log(output);
    }

    parseContextSize(value) {
        if (!value) return 8192;

        // Convert string to lowercase for case-insensitive matching
        const lower = value.toString().toLowerCase();

        // Handle 'k' suffix (4k, 8k, 32k, etc)
        if (lower.endsWith('k')) {
            return parseInt(lower.slice(0, -1)) * 1024;
        }

        // Handle plain numbers
        return parseInt(value);
    }

    parseMemory(value) {
        if (!value) return 0.9;

        const str = value.toString().toLowerCase();

        // Handle percentage (30%, 50%, etc)
        if (str.endsWith('%')) {
            return parseInt(str.slice(0, -1)) / 100;
        }

        // Handle decimal (0.3, 0.5, etc)
        const num = parseFloat(str);
        if (num > 1) {
            console.error('Memory must be between 0-1 or 0-100%');
            process.exit(1);
        }
        return num;
    }

    async handleStart(args) {
        if (!args[0]) {
            console.error('Usage: pi start <model> [options]');
            console.error('');
            console.error('Options:');
            console.error('  --name <name>      Model alias (default: auto-generated)');
            console.error('  --context <size>   Context window: 4k, 8k, 16k, 32k or 4096, 8192, etc (default: model default)');
            console.error('  --memory <percent> GPU memory: 30%, 50%, 90% or 0.3, 0.5, 0.9 (default: 90%)');
            console.error('  --all-gpus         Use all GPUs with tensor parallelism (ignores --memory)');
            console.error('  --vllm-args        Pass remaining args directly to vLLM (ignores other options)');
            console.error('');
            console.error('Examples:');
            console.error('  pi start microsoft/Phi-3-mini-128k-instruct');
            console.error('  pi start microsoft/Phi-3-mini-128k-instruct --name phi3 --memory 30%');
            console.error('  pi start meta-llama/Llama-3.1-70B-Instruct --all-gpus');
            console.error('  pi start meta-llama/Llama-3.1-405B --all-gpus --context 128k');
            console.error('');
            console.error('  # Custom vLLM args for Qwen3-Coder on 8xH200:');
            console.error('  pi start Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 --name qwen-coder --vllm-args \\');
            console.error('    --data-parallel-size 8 --enable-expert-parallel --max-model-len 131072 \\');
            console.error('    --tool-call-parser qwen3_coder --enable-auto-tool-choice');
            process.exit(1);
        }

        const modelId = args[0];
        let name = null;
        let context = null;  // Changed to null - let vLLM use model default
        let memory = 0.9;
        let allGpus = false;
        let vllmArgs = null;

        // Check for --vllm-args first
        const vllmArgsIndex = args.indexOf('--vllm-args');
        if (vllmArgsIndex !== -1) {
            // Extract name if provided before --vllm-args
            for (let i = 1; i < vllmArgsIndex; i++) {
                if (args[i] === '--name' && args[i + 1]) {
                    name = args[++i];
                }
            }
            // Everything after --vllm-args is passed to vLLM
            vllmArgs = args.slice(vllmArgsIndex + 1).join(' ');
        } else {
            // Parse normal arguments
            for (let i = 1; i < args.length; i++) {
                switch (args[i]) {
                    case '--name':
                        name = args[++i];
                        break;
                    case '--context':
                        context = this.parseContextSize(args[++i]);
                        break;
                    case '--memory':
                        memory = this.parseMemory(args[++i]);
                        break;
                    case '--all-gpus':
                        allGpus = true;
                        break;
                    default:
                        console.error(`Unknown option: ${args[i]}`);
                        process.exit(1);
                }
            }
        }

        // Check for multi-GPU setup
        const gpuCount = await this.getGpuCount();

        if (allGpus) {
            if (memory !== 0.9) {
                console.log('Warning: --memory ignored with --all-gpus (using 95% memory across all GPUs)');
            }
            memory = 0.95;

            if (gpuCount === 1) {
                console.log('Note: --all-gpus specified but only 1 GPU found');
                allGpus = false;
            }
        }

        // Auto-generate name if not provided
        if (!name) {
            // Extract model name from path (e.g., "Phi-3-mini" from "microsoft/Phi-3-mini-4k-instruct")
            const parts = modelId.split('/');
            const modelName = parts[parts.length - 1];
            name = modelName.toLowerCase()
                .replace(/-instruct$/, '')
                .replace(/-chat$/, '')
                .replace(/[^a-z0-9-]/g, '-')
                .replace(/-+/g, '-')
                .replace(/^-|-$/g, '')
                .slice(0, 20);
        }

        // If vllmArgs provided, use raw vLLM command
        if (vllmArgs) {
            await this.startRaw(modelId, name, vllmArgs);
        } else {
            // Call the original start method with positional args
            const contextStr = context ? context.toString() : null;
            await this.start(modelId, name, contextStr, memory.toString(), { allGpus, gpuCount });
        }
    }

    async getGpuCount() {
        try {
            const output = this.ssh('nvidia-smi --query-gpu=name --format=csv,noheader | wc -l');
            return parseInt(output.trim()) || 1;
        } catch {
            return 1;
        }
    }

    async start(modelId, name, maxLen = null, gpuMemory, options = {}) {
        // Check if name is already in use locally first
        if (name) {
            const runningModels = this.getRunningModels();
            if (runningModels[name]) {
                console.error(`Error: Model name '${name}' is already in use`);
                console.error('Running models:', Object.keys(runningModels).join(', '));
                process.exit(1);
            }
        }

        // Build args for vllm_manager.py
        let args = modelId;

        // Handle optional parameters
        if (name || maxLen || gpuMemory) {
            args += ` ${name || '""'}`;

            if (maxLen || gpuMemory) {
                args += ` ${maxLen || '""'}`;  // Pass empty string to use vLLM default

                if (gpuMemory) {
                    args += ` ${gpuMemory}`;
                }
            }
        }

        // Handle multi-GPU options
        let envPrefix = '';
        if (options.allGpus && options.gpuCount > 1) {
            args += ` ${options.gpuCount}`; // Pass tensor parallel size
        }

        const output = this.ssh(`${envPrefix}python3 vllm_manager.py start ${args}`);

        // Extract model name and connection info from output
        const nameMatch = output.match(/Started (\S+)/);
        const urlMatch = output.match(/URL: (http:\/\/[^\s]+)/);
        const exportMatch = output.match(/export OPENAI_BASE_URL='([^']+)'/);

        if (nameMatch) {
            const modelName = nameMatch[1];
            const url = urlMatch ? urlMatch[1] : null;
            const exportCmd = exportMatch ? `export OPENAI_BASE_URL='${exportMatch[1]}'` : null;

            console.log(`\nStarted ${modelName}`);
            console.log('Waiting for model to initialize...\n');

            // Set up Ctrl+C handler for manual interruption
            const showModelInfo = () => {
                console.log('\n\n' + '='.repeat(60));
                console.log('Model Information:');
                console.log('='.repeat(60));
                console.log(`Name: ${modelName}`);
                if (url) console.log(`URL: ${url}`);
                if (exportCmd) {
                    console.log(`\nTo use with OpenAI clients:`);
                    console.log(exportCmd);
                    console.log(`export OPENAI_API_KEY='dummy'`);
                }
                console.log('='.repeat(60));
            };

            process.on('SIGINT', () => {
                showModelInfo();
                process.exit(0);
            });

            // Watch logs until startup complete
            await this.logs(modelName);

            // Show model info after automatic exit
            showModelInfo();
        } else {
            console.log(output);
        }
    }

    async startRaw(modelId, name, vllmArgs) {
        // Check if name is already in use
        const runningModels = this.getRunningModels();
        if (runningModels[name]) {
            console.error(`Error: Model name '${name}' is already in use`);
            console.error('Running models:', Object.keys(runningModels).join(', '));
            process.exit(1);
        }

        console.log(`Starting ${name} with custom vLLM args...`);

        // Start vLLM with raw arguments - use base64 to safely pass complex args
        const base64Args = Buffer.from(vllmArgs).toString('base64');
        const output = this.ssh(`python3 vllm_manager.py start_raw "${modelId}" "${name}" "${base64Args}"`);

        // Extract connection info from output
        const urlMatch = output.match(/URL: (http:\/\/[^\s]+)/);
        const exportMatch = output.match(/export OPENAI_BASE_URL='([^']+)'/);

        if (urlMatch || exportMatch) {
            const url = urlMatch ? urlMatch[1] : null;
            const exportCmd = exportMatch ? `export OPENAI_BASE_URL='${exportMatch[1]}'` : null;

            console.log(`\nStarted ${name}`);
            console.log('Waiting for model to initialize...\n');

            // Set up Ctrl+C handler for manual interruption
            const showModelInfo = () => {
                console.log('\n\n' + '='.repeat(60));
                console.log('Model Information:');
                console.log('='.repeat(60));
                console.log(`Name: ${name}`);
                if (url) console.log(`URL: ${url}`);
                if (exportCmd) {
                    console.log(`\nTo use with OpenAI clients:`);
                    console.log(exportCmd);
                    console.log(`export OPENAI_API_KEY='dummy'`);
                }
                console.log('='.repeat(60));
            };

            process.on('SIGINT', () => {
                showModelInfo();
                process.exit(0);
            });

            // Watch logs until startup complete
            await this.logs(name);

            // Show model info after automatic exit
            showModelInfo();
        } else {
            console.log(output);
        }
    }

    stop(name) {
        if (!name) {
            // Stop all models
            const runningModels = this.getRunningModels();
            const modelNames = Object.keys(runningModels);

            if (modelNames.length === 0) {
                console.log('No models running');
                return;
            }

            console.log(`Stopping ${modelNames.length} model(s): ${modelNames.join(', ')}`);

            for (const modelName of modelNames) {
                const output = this.ssh(`python3 vllm_manager.py stop ${modelName}`);
                console.log(output);
            }
        } else {
            // Stop specific model
            const output = this.ssh(`python3 vllm_manager.py stop ${name}`);
            console.log(output);
        }
    }

    async logs(name) {
        if (!name) {
            console.error('Usage: pi logs <name>');
            process.exit(1);
        }

        // Use vllm_manager.py to get the log file path
        const infoOutput = this.ssh(`python3 vllm_manager.py list`);

        // Extract log file path from the output
        const lines = infoOutput.split('\n');
        let logFile = null;
        let inModel = false;

        for (const line of lines) {
            if (line.startsWith(`${name}:`)) {
                inModel = true;
            } else if (inModel && line.includes('Logs:')) {
                logFile = line.split('Logs:')[1].trim();
                break;
            }
        }

        if (!logFile) {
            console.error(`No logs found for ${name}`);
            process.exit(1);
        }

        // Use a custom tail that watches for startup complete
        const pod = this.getActivePod();
        const sshCmd = `ssh ${pod.ssh} tail -n 50 -f ${logFile}`;

        return new Promise((resolve) => {
            const [cmd, ...args] = sshCmd.split(' ');
            const proc = spawn(cmd, args, { stdio: ['inherit', 'pipe', 'pipe'] });

            let buffer = '';

            proc.stdout.on('data', (data) => {
                process.stdout.write(data);
                buffer += data.toString();

                // Check for vLLM startup complete message
                if (buffer.includes('Application startup complete.') ||
                    buffer.includes('Uvicorn running on')) {
                    setTimeout(() => {
                        proc.kill();
                        resolve();
                    }, 500); // Small delay to ensure final messages are shown
                }

                // Keep buffer size manageable
                if (buffer.length > 10000) {
                    buffer = buffer.slice(-5000);
                }
            });

            proc.stderr.on('data', (data) => {
                process.stderr.write(data);
            });

            proc.on('close', () => {
                resolve();
            });
        });
    }

    async shell() {
        const pod = this.getActivePod();
        if (!pod) {
            console.error('No active pod. Run: pi setup <pod-name> <ssh_command>');
            process.exit(1);
        }

        console.log('Connecting to pod...');

        // Use spawn directly for interactive shell
        const sshParts = pod.ssh.split(' ');
        const sshCmd = ['ssh', ...sshParts];
        const proc = spawn(sshCmd[0], sshCmd.slice(1), { stdio: 'inherit' });

        return new Promise((resolve) => {
            proc.on('close', resolve);
        });
    }

    listPods() {
        if (!this.config.pods || Object.keys(this.config.pods).length === 0) {
            console.log('No pods configured. Run: pi setup <pod-name> <ssh_command>');
            return;
        }

        console.log('Configured pods:\n');

        // Show active pod first
        if (this.config.active && this.config.pods[this.config.active]) {
            console.log(`● ${this.config.active} (active)`);
            console.log(`  ${this.config.pods[this.config.active].ssh}\n`);
        }

        // Show other pods
        Object.keys(this.config.pods).sort().forEach(name => {
            if (name !== this.config.active) {
                console.log(`○ ${name}`);
                console.log(`  ${this.config.pods[name].ssh}`);
            }
        });
    }

    switchPod(podName) {
        if (!this.config.pods || !this.config.pods[podName]) {
            console.error(`Pod '${podName}' not found`);
            console.error('Available pods:', Object.keys(this.config.pods || {}).join(', ') || 'none');
            process.exit(1);
        }

        this.config.active = podName;
        this.saveConfig();
        console.log(`Switched to pod: ${podName} (${this.config.pods[podName].ssh})`);
    }

    removePod(podName) {
        if (!this.config.pods || !this.config.pods[podName]) {
            console.error(`Pod '${podName}' not found`);
            console.error('Available pods:', Object.keys(this.config.pods || {}).join(', ') || 'none');
            process.exit(1);
        }

        delete this.config.pods[podName];

        // If we removed the active pod, clear it or switch to another
        if (this.config.active === podName) {
            const remainingPods = Object.keys(this.config.pods);
            this.config.active = remainingPods.length > 0 ? remainingPods[0] : null;
        }

        this.saveConfig();
        console.log(`Removed pod: ${podName}`);
        if (this.config.active) {
            console.log(`Active pod is now: ${this.config.active}`);
        }
    }

    async searchModels(query) {
        console.log(`Searching HuggingFace for models matching "${query}"...\n`);

        try {
            const response = await fetch(`https://huggingface.co/api/models?search=${query}&filter=text-generation&sort=downloads&limit=20`);
            const data = await response.json();

            if (!data || data.length === 0) {
                console.log('No models found');
                return;
            }

            // Format results
            console.log('Popular models (sorted by downloads):\n');
            for (const model of data) {
                const modelName = model.modelId.toLowerCase();

                // Skip incompatible formats
                if (modelName.includes('-mlx-') || modelName.includes('-mlx')) {
                    continue; // MLX is for Apple Silicon only
                }
                if (modelName.includes('-gguf') || modelName.includes('.gguf')) {
                    continue; // GGUF is for llama.cpp, not vLLM
                }

                const downloads = model.downloads || 0;
                const likes = model.likes || 0;

                console.log(`\x1b[1m${model.modelId}\x1b[0m`); // Bold
                console.log(`  \x1b[36mhttps://huggingface.co/${model.modelId}\x1b[0m`); // Cyan for URL
                console.log(`  Downloads: ${downloads.toLocaleString()} | Likes: ${likes}`);

                // Check for quantization
                if (modelName.includes('-fp8') || modelName.includes('fp8-')) {
                    console.log(`  \x1b[33mNote: FP8 quantized - requires GPU with FP8 support\x1b[0m`);
                }

                console.log(`  pi start ${model.modelId}`);
                console.log();
            }

            // Add HuggingFace search URL
            console.log(`\nView more models on HuggingFace:`);
            console.log(`\x1b[36mhttps://huggingface.co/models?search=${encodeURIComponent(query)}&sort=downloads&pipeline_tag=text-generation\x1b[0m`);
        } catch (error) {
            console.error('Error searching models:', error.message);
        }
    }

    async prompt(name, message) {
        // Get model info
        const models = this.getRunningModels();
        const model = models[name];

        if (!model) {
            console.error(`Model '${name}' is not running`);
            console.error('Running models:', Object.keys(models).join(', ') || 'none');
            process.exit(1);
        }

        // Get pod IP
        const output = this.ssh('hostname -I').trim();
        const podIp = output.split(' ')[0];

        // Make API call
        const url = `http://${podIp}:${model.port}/v1/chat/completions`;
        const payload = {
            model: model.model_id,
            messages: [{ role: 'user', content: message }],
            max_tokens: 500,
            temperature: 0.7
        };

        try {
            const response = await fetch(url, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });

            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${await response.text()}`);
            }

            const data = await response.json();
            console.log(data.choices[0].message.content);
        } catch (error) {
            console.error('Error:', error.message);
            process.exit(1);
        }
    }

    showHelp() {
        console.log('\nPrime Intellect CLI\n');

        console.log('Pod Management:');
        console.log('  pi setup <pod-name> <ssh_command>  Configure and activate a pod');
        console.log('  pi pods                            List all pods (active pod marked)');
        console.log('  pi pod <pod-name>                  Switch active pod');
        console.log('  pi pod remove <pod-name>           Remove pod from config\n');
        console.log('Model Management:');
        console.log('  pi list                            List running models');
        console.log('  pi search <query>                  Search HuggingFace models');
        console.log('  pi start <model> [options]         Start a model');
        console.log('  pi stop [name]                     Stop a model (or all if no name)');
        console.log('  pi logs <name>                     View model logs');
        console.log('  pi prompt <name> <msg>             Chat with a model\n');
        console.log('Start Options:');
        console.log('  --name <name>      Model alias (default: auto-generated)');
        console.log('  --context <size>   Context window: 4k, 16k, 32k (default: 8k)');
        console.log('  --memory <percent> GPU memory: 30%, 50%, 90% (default: 90%)');
        console.log('  --all-gpus         Use all GPUs with tensor parallelism\n');
        console.log('Utility:');
        console.log('  pi shell                           SSH into active pod');

        console.log('\nQuick Example:');
        console.log('  pi start microsoft/Phi-3-mini-128k-instruct --name phi3 --memory 30%');
        console.log('  pi prompt phi3 "What is 2+2?"');

        if (this.config.active && this.config.pods[this.config.active]) {
            console.log(`\nActive pod: ${this.config.active} (${this.config.pods[this.config.active].ssh})`);
        } else {
            console.log('\nNo active pod');
        }
    }

    getRunningModels() {
        try {
            const output = this.ssh('python3 vllm_manager.py list');
            const models = {};

            // Parse the output to extract model info
            const lines = output.split('\n');
            let currentModel = null;

            for (const line of lines) {
                if (line.match(/^[a-zA-Z0-9_-]+:$/)) {
                    currentModel = line.slice(0, -1);
                } else if (currentModel && line.includes('Model:')) {
                    const modelId = line.split('Model:')[1].trim();
                    models[currentModel] = { model_id: modelId };
                } else if (currentModel && line.includes('Port:')) {
                    const port = parseInt(line.split('Port:')[1].trim());
                    if (models[currentModel]) {
                        models[currentModel].port = port;
                    }
                }
            }

            return models;
        } catch (e) {
            return {};
        }
    }

    async run() {
        const [,, command, ...args] = process.argv;

        switch (command) {
            case 'setup': {
                if (args.length < 2) {
                    console.error('Usage: pi setup <pod-name> <ssh_command>');
                    console.error('Example: pi setup prod "root@135.181.71.41 -p 22"');
                    process.exit(1);
                }
                const podName = args[0];
                const sshCmd = args.slice(1).join(' ');
                this.setup(podName, sshCmd);
                break;
            }
            case 'pods':
                this.listPods();
                break;

            case 'pod':
                if (!args[0]) {
                    console.error('Usage: pi pod <pod-name>');
                    console.error('       pi pod remove <pod-name>');
                    process.exit(1);
                }
                if (args[0] === 'remove' && args[1]) {
                    this.removePod(args[1]);
                } else {
                    this.switchPod(args[0]);
                }
                break;

            case 'list':
            case 'ls':
                this.list();
                break;

            case 'search':
                if (!args[0]) {
                    console.error('Usage: pi search <query>');
                    console.error('Example: pi search qwen');
                    process.exit(1);
                }
                await this.searchModels(args[0]);
                break;

            case 'start':
                await this.handleStart(args);
                break;

            case 'stop':
                this.stop(args[0]);
                break;

            case 'logs':
                await this.logs(args[0]);
                break;

            case 'prompt': {
                if (args.length < 2) {
                    console.error('Usage: pi prompt <model_name> "<message>"');
                    console.error('Example: pi prompt phi3 "Hey, how you going"');
                    process.exit(1);
                }
                const modelName = args[0];
                const message = args.slice(1).join(' ');
                this.prompt(modelName, message);
                break;
            }
            case 'shell':
                await this.shell();
                break;

            case 'ssh':
                // Pass through any SSH command
                if (args.length > 0) {
                    const output = this.ssh(args.join(' '));
                    console.log(output);
                } else {
                    this.shell();
                }
                break;

            default:
                this.showHelp();
        }
    }
}

// Run CLI
const cli = new PrimeIntellectCLI();
cli.run().catch(console.error);